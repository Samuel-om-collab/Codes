{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tDiIkWnGvzwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "Owu9iBlP9yk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this data preparation process for sentiment analysis using the IMDB dataset, we begin by setting two key hyperparameters: max_features to 10,000, limiting our vocabulary to the 10,000 most frequent words, and max_len to 200, ensuring each review is truncated or padded to 200 words for uniformity. We then load the IMDB dataset, which contains movie reviews encoded as sequences of integers representing word indices. By specifying num_words=max_features, we retain only the top 10,000 most frequent words, with less frequent words replaced by an out-of-vocabulary token. Following this, we standardize the length of each review using Keras's pad_sequences function, which pads shorter sequences with zeros and truncates longer ones to ensure all inputs are of length 200. This uniformity is crucial for feeding data into neural network models, as it ensures consistent input dimensions and facilitates efficient processing."
      ],
      "metadata": {
        "id": "gVNyTI-dvuxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "# Load the IMDB dataset\n",
        "max_features = 10000 # Number of words to consideras features\n",
        "max_len = 200 # Trim reviews after ths number of words\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "# Pad sequences to a fixed length\n",
        "X = train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwHuiAm_fMg",
        "outputId": "dacc2f9d-fc16-4f4b-e6d4-046bee7a8899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aDf-VUtnv1pL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "​In this data preparation and modeling workflow for sentiment analysis using the IMDB dataset, we begin by defining two key hyperparameters: max_features is set to 10,000, restricting our vocabulary to the 10,000 most frequent words, and max_len is set to 200, ensuring each review is truncated or padded to 200 words for uniformity. We then load the IMDB dataset, which contains movie reviews encoded as sequences of integers representing word indices, retaining only the top 10,000 most frequent words. To standardize input lengths, we utilize Keras's pad_sequences function to pad shorter sequences with zeros and truncate longer ones, resulting in all reviews having a uniform length of 200 words. This uniformity is crucial for feeding data into neural network models, as it ensures consistent input dimensions and facilitates efficient processing. Subsequently, we construct a Gated Recurrent Unit (GRU) model using Keras's Sequential API, comprising three primary layers: an Embedding layer that converts input integers into dense 128-dimensional vectors, allowing the model to work with continuous input representations; a GRU layer with 128 units, designed to capture temporal dependencies in sequential data, with dropout and recurrent dropout rates set to 0.2 to prevent overfitting; and a Dense output layer with a sigmoid activation function that outputs a single value between 0 and 1, making it suitable for binary classification tasks. By stacking these layers, the model can process sequences of word indices, capture their temporal relationships, and produce predictions, such as determining the sentiment of a movie review."
      ],
      "metadata": {
        "id": "prf5ZxR0v5F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the GRU model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(max_features, 128), # Added comma here\n",
        "    layers.GRU(128, dropout=0.2, recurrent_dropout=0.2), # Added comma here\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "]) # This is the correct closing parenthesis for the list"
      ],
      "metadata": {
        "id": "Lkp0XtQaAGH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "​In this phase of the sentiment analysis project, we focus on compiling and training the GRU (Gated Recurrent Unit) model using the IMDB dataset. We begin by compiling the model with the binary cross-entropy loss function, suitable for binary classification tasks, and the Adam optimizer, known for its efficiency in training deep learning models. Additionally, we specify 'accuracy' as the evaluation metric to monitor the model's performance during training. Training is conducted over five epochs with a batch size of 32, meaning the entire dataset is passed through the model five times, and updates to model weights occur after processing each batch of 32 samples. This approach aims to optimize the model's ability to accurately predict the sentiment of movie reviews."
      ],
      "metadata": {
        "id": "ZNcc_x79v_Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "model.fit(X, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ5Hc_WYAI4v",
        "outputId": "a274e528-5ad8-43f9-9658-98df0b980b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 644ms/step - accuracy: 0.6794 - loss: 0.5762 - val_accuracy: 0.8558 - val_loss: 0.3372\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 646ms/step - accuracy: 0.8934 - loss: 0.2668 - val_accuracy: 0.8875 - val_loss: 0.2685\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 643ms/step - accuracy: 0.9417 - loss: 0.1552 - val_accuracy: 0.8757 - val_loss: 0.3057\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 642ms/step - accuracy: 0.9659 - loss: 0.0983 - val_accuracy: 0.8735 - val_loss: 0.3536\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 635ms/step - accuracy: 0.9806 - loss: 0.0600 - val_accuracy: 0.8643 - val_loss: 0.4324\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x786ce78ddb90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the model, we proceed to evaluate its performance on unseen data to assess its generalization capability. This is achieved using the model.evaluate() function, which computes the model's loss and accuracy on the test dataset. The function returns two primary metrics:​\n",
        "\n",
        "Loss: A scalar value representing the model's error on the test data. Lower loss indicates better model performance.​\n",
        "\n",
        "Accuracy: The proportion of correctly predicted instances out of the total instances, reflecting the model's predictive capability.​\n",
        "\n",
        "By printing these metrics, we gain insights into how well the model is likely to perform on new, unseen data. This evaluation is crucial for understanding the model's effectiveness and identifying potential areas for improvement."
      ],
      "metadata": {
        "id": "0yukjl4qwEAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test,)\n",
        "print(f'Test loss: {loss:.4f}')\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "sGTzMD7nBRo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}